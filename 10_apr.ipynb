{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a1dd7c7-b61e-4379-8707-acd56d6f8432",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2727f8f2-a76b-45c4-94b4-c6501224d668",
   "metadata": {},
   "source": [
    "Answer--> To find the probability that an employee is a smoker given that they use the health insurance plan, we can use the conditional probability formula:\n",
    "\n",
    "[ P(Smoker | Uses Health Insurance) = P(Smoker and Uses Health Insurance) \\ P(Uses Health Insurance)]\n",
    "\n",
    "Given the information:\n",
    "- \\( P(Uses Health Insurance) = 0.70 \\) (70% of employees use the health insurance plan)\n",
    "- \\( P(Smoker) = 0.40 \\) (40% of employees who use the plan are smokers)\n",
    "- \\( P(Uses Health Insurance | Smoker) = 1 \\) (all smokers use the health insurance plan)\n",
    "\n",
    "We want to calculate ( P(Smoker | Uses Health Insurance)).\n",
    "\n",
    "First, calculate P(Smoker and Uses Health Insurance):\n",
    "\\[ P(Smoker and Uses Health Insurance) = P(Smoker) * P(Uses Health Insurance | Smoker) = 0.40 * 1 = 0.40 \\]\n",
    "\n",
    "Now, calculate ( P(Smoker | Uses Health Insurance)):\n",
    "\\[ P(Smoker | Uses Health Insurance) = {P(Smoker  and Uses Health Insurance)}/{P(Uses Health Insurance)} = {0.40}/{0.70} = 0.5714 \\]\n",
    "\n",
    "So, the probability that an employee is a smoker given that they use the health insurance plan is approximately \\(0.5714\\) or \\(57.14\\%\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceb1175-e903-4116-adaa-915b03bcd2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e404ac51-6ada-4ae4-94ab-37c2895d2d97",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "Answer--> Here's the difference between the two:\n",
    "\n",
    "1. **Bernoulli Naive Bayes**:\n",
    "   - **Input Data**: Bernoulli Naive Bayes is typically used for binary feature data, where each feature represents the presence or absence of a particular word or term. It's commonly applied in text classification tasks where the focus is on whether a word appears in a document or not.\n",
    "   - **Feature Representation**: Each feature is binary, representing the presence (1) or absence (0) of a specific term in a document.\n",
    "   - **Example**: Email spam detection, sentiment analysis (where the presence or absence of certain words in a text is important).\n",
    "\n",
    "2. **Multinomial Naive Bayes**:\n",
    "   - **Input Data**: Multinomial Naive Bayes is used for data with discrete counts, often representing word frequencies or term occurrences. It's suitable for text classification tasks where features are counts of words or terms in a document.\n",
    "   - **Feature Representation**: Features are represented by counts of occurrences (non-negative integers).\n",
    "   - **Example**: Document classification, topic categorization, spam filtering (where the frequency of words matters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40423c-0d3c-4e42-8ce1-f1556fc5118d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b938da5-b332-4f99-8bd9-89132ae9d64a",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "Answer--> In Bernoulli Naive Bayes, which deals with binary features (0 or 1), handling missing values can be approached in a couple of ways:\n",
    "\n",
    "1. **Ignoring Missing Values**: Treat instances with missing values as if the corresponding feature is absent (0). This assumes missing values don't carry unique information.\n",
    "\n",
    "2. **Imputing Missing Values**: You can replace missing values with a default value, typically 0, to indicate absence. However, this approach needs careful consideration to avoid introducing bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5ae55-5a8c-4de0-ab4f-8832e4adf83d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9486f4ac-1edb-48d0-951a-9a7532e1fef7",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "Answer--> Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes continuous features follow a Gaussian (normal) distribution. While it's commonly used for binary classification, it can also be extended to handle multi-class classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f1c54-7678-4509-9bd1-379c23c1089f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91efd459-80c8-4959-8ed5-7c140a826fac",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "\n",
    "Data preparation:Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84a45264-6f30-4a97-a338-f43d0d661c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  word_freq_make word_freq_address word_freq_all word_freq_3d word_freq_our  \\\n",
       "0              0              0.64          0.64            0          0.32   \n",
       "1           0.21              0.28           0.5            0          0.14   \n",
       "2           0.06                 0          0.71            0          1.23   \n",
       "3              0                 0             0            0          0.63   \n",
       "4              0                 0             0            0          0.63   \n",
       "\n",
       "  word_freq_over word_freq_remove word_freq_internet word_freq_order  \\\n",
       "0              0                0                  0               0   \n",
       "1           0.28             0.21               0.07               0   \n",
       "2           0.19             0.19               0.12            0.64   \n",
       "3              0             0.31               0.63            0.31   \n",
       "4              0             0.31               0.63            0.31   \n",
       "\n",
       "  word_freq_mail  ... char_freq_; char_freq_( char_freq_[ char_freq_!  \\\n",
       "0              0  ...           0           0           0       0.778   \n",
       "1           0.94  ...           0       0.132           0       0.372   \n",
       "2           0.25  ...        0.01       0.143           0       0.276   \n",
       "3           0.63  ...           0       0.137           0       0.137   \n",
       "4           0.63  ...           0       0.135           0       0.135   \n",
       "\n",
       "  char_freq_$ char_freq_# capital_run_length_average  \\\n",
       "0           0           0                      3.756   \n",
       "1        0.18       0.048                      5.114   \n",
       "2       0.184        0.01                      9.821   \n",
       "3           0           0                      3.537   \n",
       "4           0           0                      3.537   \n",
       "\n",
       "  capital_run_length_longest capital_run_length_total spam  \n",
       "0                         61                      278    1  \n",
       "1                        101                     1028    1  \n",
       "2                        485                     2259    1  \n",
       "3                         40                      191    1  \n",
       "4                         40                      191    1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load Column Names from .names File\n",
    "column_names = []\n",
    "with open('spam.names', 'r') as names_file:\n",
    "    for line in names_file:\n",
    "        if line.startswith('|'):\n",
    "            continue\n",
    "        if ':' in line:\n",
    "            column_name = line.split(':')[0].strip()\n",
    "            column_names.append(column_name)\n",
    "\n",
    "# Step 2: Load Data from .data File\n",
    "data = []\n",
    "with open('spam.data', 'r') as data_file:\n",
    "    for line in data_file :\n",
    "        values = line.strip().split(',')\n",
    "        data.append(values)\n",
    "\n",
    "# Step 3: Create Pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=column_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0b35b9-eb5e-4b0e-aec1-88303b04896a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4601 entries, 0 to 4600\n",
      "Data columns (total 58 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   word_freq_make              4601 non-null   object\n",
      " 1   word_freq_address           4601 non-null   object\n",
      " 2   word_freq_all               4601 non-null   object\n",
      " 3   word_freq_3d                4601 non-null   object\n",
      " 4   word_freq_our               4601 non-null   object\n",
      " 5   word_freq_over              4601 non-null   object\n",
      " 6   word_freq_remove            4601 non-null   object\n",
      " 7   word_freq_internet          4601 non-null   object\n",
      " 8   word_freq_order             4601 non-null   object\n",
      " 9   word_freq_mail              4601 non-null   object\n",
      " 10  word_freq_receive           4601 non-null   object\n",
      " 11  word_freq_will              4601 non-null   object\n",
      " 12  word_freq_people            4601 non-null   object\n",
      " 13  word_freq_report            4601 non-null   object\n",
      " 14  word_freq_addresses         4601 non-null   object\n",
      " 15  word_freq_free              4601 non-null   object\n",
      " 16  word_freq_business          4601 non-null   object\n",
      " 17  word_freq_email             4601 non-null   object\n",
      " 18  word_freq_you               4601 non-null   object\n",
      " 19  word_freq_credit            4601 non-null   object\n",
      " 20  word_freq_your              4601 non-null   object\n",
      " 21  word_freq_font              4601 non-null   object\n",
      " 22  word_freq_000               4601 non-null   object\n",
      " 23  word_freq_money             4601 non-null   object\n",
      " 24  word_freq_hp                4601 non-null   object\n",
      " 25  word_freq_hpl               4601 non-null   object\n",
      " 26  word_freq_george            4601 non-null   object\n",
      " 27  word_freq_650               4601 non-null   object\n",
      " 28  word_freq_lab               4601 non-null   object\n",
      " 29  word_freq_labs              4601 non-null   object\n",
      " 30  word_freq_telnet            4601 non-null   object\n",
      " 31  word_freq_857               4601 non-null   object\n",
      " 32  word_freq_data              4601 non-null   object\n",
      " 33  word_freq_415               4601 non-null   object\n",
      " 34  word_freq_85                4601 non-null   object\n",
      " 35  word_freq_technology        4601 non-null   object\n",
      " 36  word_freq_1999              4601 non-null   object\n",
      " 37  word_freq_parts             4601 non-null   object\n",
      " 38  word_freq_pm                4601 non-null   object\n",
      " 39  word_freq_direct            4601 non-null   object\n",
      " 40  word_freq_cs                4601 non-null   object\n",
      " 41  word_freq_meeting           4601 non-null   object\n",
      " 42  word_freq_original          4601 non-null   object\n",
      " 43  word_freq_project           4601 non-null   object\n",
      " 44  word_freq_re                4601 non-null   object\n",
      " 45  word_freq_edu               4601 non-null   object\n",
      " 46  word_freq_table             4601 non-null   object\n",
      " 47  word_freq_conference        4601 non-null   object\n",
      " 48  char_freq_;                 4601 non-null   object\n",
      " 49  char_freq_(                 4601 non-null   object\n",
      " 50  char_freq_[                 4601 non-null   object\n",
      " 51  char_freq_!                 4601 non-null   object\n",
      " 52  char_freq_$                 4601 non-null   object\n",
      " 53  char_freq_#                 4601 non-null   object\n",
      " 54  capital_run_length_average  4601 non-null   object\n",
      " 55  capital_run_length_longest  4601 non-null   object\n",
      " 56  capital_run_length_total    4601 non-null   object\n",
      " 57  spam                        4601 non-null   object\n",
      "dtypes: object(58)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c72e0b37-4264-4e7e-b71e-f201e1f68bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>...</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>142</td>\n",
       "      <td>171</td>\n",
       "      <td>214</td>\n",
       "      <td>43</td>\n",
       "      <td>255</td>\n",
       "      <td>141</td>\n",
       "      <td>173</td>\n",
       "      <td>170</td>\n",
       "      <td>144</td>\n",
       "      <td>245</td>\n",
       "      <td>...</td>\n",
       "      <td>313</td>\n",
       "      <td>641</td>\n",
       "      <td>225</td>\n",
       "      <td>964</td>\n",
       "      <td>504</td>\n",
       "      <td>316</td>\n",
       "      <td>2161</td>\n",
       "      <td>271</td>\n",
       "      <td>919</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3548</td>\n",
       "      <td>3703</td>\n",
       "      <td>2713</td>\n",
       "      <td>4554</td>\n",
       "      <td>2853</td>\n",
       "      <td>3602</td>\n",
       "      <td>3794</td>\n",
       "      <td>3777</td>\n",
       "      <td>3828</td>\n",
       "      <td>3299</td>\n",
       "      <td>...</td>\n",
       "      <td>3811</td>\n",
       "      <td>1886</td>\n",
       "      <td>4072</td>\n",
       "      <td>2343</td>\n",
       "      <td>3201</td>\n",
       "      <td>3851</td>\n",
       "      <td>349</td>\n",
       "      <td>349</td>\n",
       "      <td>115</td>\n",
       "      <td>2788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_freq_make word_freq_address word_freq_all word_freq_3d  \\\n",
       "count            4601              4601          4601         4601   \n",
       "unique            142               171           214           43   \n",
       "top                 0                 0             0            0   \n",
       "freq             3548              3703          2713         4554   \n",
       "\n",
       "       word_freq_our word_freq_over word_freq_remove word_freq_internet  \\\n",
       "count           4601           4601             4601               4601   \n",
       "unique           255            141              173                170   \n",
       "top                0              0                0                  0   \n",
       "freq            2853           3602             3794               3777   \n",
       "\n",
       "       word_freq_order word_freq_mail  ... char_freq_; char_freq_(  \\\n",
       "count             4601           4601  ...        4601        4601   \n",
       "unique             144            245  ...         313         641   \n",
       "top                  0              0  ...           0           0   \n",
       "freq              3828           3299  ...        3811        1886   \n",
       "\n",
       "       char_freq_[ char_freq_! char_freq_$ char_freq_#  \\\n",
       "count         4601        4601        4601        4601   \n",
       "unique         225         964         504         316   \n",
       "top              0           0           0           0   \n",
       "freq          4072        2343        3201        3851   \n",
       "\n",
       "       capital_run_length_average capital_run_length_longest  \\\n",
       "count                        4601                       4601   \n",
       "unique                       2161                        271   \n",
       "top                             1                          1   \n",
       "freq                          349                        349   \n",
       "\n",
       "       capital_run_length_total  spam  \n",
       "count                      4601  4601  \n",
       "unique                      919     2  \n",
       "top                           5     0  \n",
       "freq                        115  2788  \n",
       "\n",
       "[4 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae3b331e-f790-4396-8478-1fbaaf32b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type casting the col into float \n",
    "\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0660878-08de-44de-8d87-fecaf2de69ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "count     4601.000000        4601.000000    4601.000000   4601.000000   \n",
       "mean         0.104553           0.213015       0.280656      0.065425   \n",
       "std          0.305358           1.290575       0.504143      1.395151   \n",
       "min          0.000000           0.000000       0.000000      0.000000   \n",
       "25%          0.000000           0.000000       0.000000      0.000000   \n",
       "50%          0.000000           0.000000       0.000000      0.000000   \n",
       "75%          0.000000           0.000000       0.420000      0.000000   \n",
       "max          4.540000          14.280000       5.100000     42.810000   \n",
       "\n",
       "       word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "count    4601.000000     4601.000000       4601.000000         4601.000000   \n",
       "mean        0.312223        0.095901          0.114208            0.105295   \n",
       "std         0.672513        0.273824          0.391441            0.401071   \n",
       "min         0.000000        0.000000          0.000000            0.000000   \n",
       "25%         0.000000        0.000000          0.000000            0.000000   \n",
       "50%         0.000000        0.000000          0.000000            0.000000   \n",
       "75%         0.380000        0.000000          0.000000            0.000000   \n",
       "max        10.000000        5.880000          7.270000           11.110000   \n",
       "\n",
       "       word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "count      4601.000000     4601.000000  ...  4601.000000  4601.000000   \n",
       "mean          0.090067        0.239413  ...     0.038575     0.139030   \n",
       "std           0.278616        0.644755  ...     0.243471     0.270355   \n",
       "min           0.000000        0.000000  ...     0.000000     0.000000   \n",
       "25%           0.000000        0.000000  ...     0.000000     0.000000   \n",
       "50%           0.000000        0.000000  ...     0.000000     0.065000   \n",
       "75%           0.000000        0.160000  ...     0.000000     0.188000   \n",
       "max           5.260000       18.180000  ...     4.385000     9.752000   \n",
       "\n",
       "       char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.016976     0.269071     0.075811     0.044238   \n",
       "std       0.109394     0.815672     0.245882     0.429342   \n",
       "min       0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.315000     0.052000     0.000000   \n",
       "max       4.081000    32.478000     6.003000    19.829000   \n",
       "\n",
       "       capital_run_length_average  capital_run_length_longest  \\\n",
       "count                 4601.000000                 4601.000000   \n",
       "mean                     5.191515                   52.172789   \n",
       "std                     31.729449                  194.891310   \n",
       "min                      1.000000                    1.000000   \n",
       "25%                      1.588000                    6.000000   \n",
       "50%                      2.276000                   15.000000   \n",
       "75%                      3.706000                   43.000000   \n",
       "max                   1102.500000                 9989.000000   \n",
       "\n",
       "       capital_run_length_total         spam  \n",
       "count               4601.000000  4601.000000  \n",
       "mean                 283.289285     0.394045  \n",
       "std                  606.347851     0.488698  \n",
       "min                    1.000000     0.000000  \n",
       "25%                   35.000000     0.000000  \n",
       "50%                   95.000000     0.000000  \n",
       "75%                  266.000000     1.000000  \n",
       "max                15841.000000     1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8120c2-d141-4498-9073-c6da8a5ec0b3",
   "metadata": {},
   "source": [
    "## Model traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4ca2be2-4ba5-4405-8074-28b1efb9843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(\"spam\", axis = 1)\n",
    "y = df.spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dd221d0-8420-4ee0-bfae-9e048ee935b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB Classifier:\n",
      " - accuracy: 0.8839\n",
      " - precision: 0.8870\n",
      " - recall: 0.8152\n",
      " - f1: 0.8481\n",
      "------------------------------\n",
      "Multinomial NB Classifier:\n",
      " - accuracy: 0.7863\n",
      " - precision: 0.7393\n",
      " - recall: 0.7215\n",
      " - f1: 0.7283\n",
      "------------------------------\n",
      "Gaussian NB Classifier:\n",
      " - accuracy: 0.8218\n",
      " - precision: 0.7104\n",
      " - recall: 0.9570\n",
      " - f1: 0.8131\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Initialize classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Perform 10-fold cross-validation and calculate metrics\n",
    "classifiers = [('Bernoulli NB', bernoulli_nb), ('Multinomial NB', multinomial_nb), ('Gaussian NB', gaussian_nb)]\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "for classifier_name, classifier in classifiers:\n",
    "    print(f\"{classifier_name} Classifier:\")\n",
    "    for metric in metrics:\n",
    "        scores = cross_val_score(classifier, x, y, cv=10, scoring=metric)\n",
    "        avg_score = scores.mean()\n",
    "        print(f\" - {metric}: {avg_score:.4f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b111c3-bb0b-4a6f-aa9a-b226f7f5a688",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "Based on the performance metrics, the Bernoulli Naive Bayes classifier performed the best among the three variants on this dataset. It achieved the highest accuracy, precision, and F1 score, along with a high recall rate. This could be because the dataset consists of binary features, making Bernoulli Naive Bayes well-suited for such data.\n",
    "\n",
    "Limited Expressive Power: Naive Bayes is a simple model and might not capture complex relationships in the data.\n",
    "\n",
    "Conclusion:\n",
    "In conclusion, this analysis provides valuable insights into the performance of different Naive Bayes variants on the spam classification task. By exploring the suggestions for future work, researchers and practitioners can further enhance the classification accuracy and gain a deeper understanding of the dataset's characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07636df9-bdcf-4a21-9422-e503e47c07f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649ca7c3-6f23-4e97-bc19-411bc086441b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
